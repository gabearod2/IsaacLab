--- git status ---
On branch rl_deployment
Your branch is up to date with 'origin/rl_deployment'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   logs/rsl_rl/unitree_go2_flat/2024-07-16_15-58-34/exported/policy.pt
	modified:   source/extensions/omni.isaac.lab_assets/omni/isaac/lab_assets/unitree.py
	modified:   source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
	modified:   source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py
	modified:   source/standalone/workflows/rsl_rl/play.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	logs/rsl_rl/unitree_go2_flat/2024-07-16_15-58-34/exported/policy.onnx
	logs/rsl_rl/unitree_go2_flat/2024-07-16_17-30-39/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_11-42-40/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_11-50-41/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_11-58-58/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_12-10-05/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_12-19-30/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_12-24-51/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_12-34-48/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_12-45-39/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_12-55-47/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_13-13-35/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_13-25-52/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_13-32-38/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_13-41-32/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_13-50-17/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_13-59-56/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_14-09-39/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_14-24-05/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_14-28-49/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_14-43-49/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_14-53-21/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_15-02-45/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_15-12-56/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_15-25-21/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_15-36-49/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_15-49-58/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_15-55-56/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_16-12-40/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_16-27-36/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_16-39-58/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_16-49-32/
	logs/rsl_rl/unitree_go2_flat/2024-07-17_17-05-19/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_10-50-57/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_10-57-11/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_11-03-41/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_11-46-35/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_11-53-14/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_11-59-32/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_12-10-03/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_12-16-29/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_12-24-30/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_12-30-35/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_12-39-59/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_12-48-04/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_12-54-14/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_13-01-00/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_15-09-27/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_15-15-30/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_15-16-30/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_15-24-26/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_15-30-31/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_15-37-22/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_15-43-54/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_15-51-26/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_15-57-47/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_16-08-56/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_16-24-20/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_16-30-37/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_16-47-16/
	logs/rsl_rl/unitree_go2_flat/2024-07-18_16-56-16/

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/logs/rsl_rl/unitree_go2_flat/2024-07-16_15-58-34/exported/policy.pt b/logs/rsl_rl/unitree_go2_flat/2024-07-16_15-58-34/exported/policy.pt
index 0b3eb55..e5fe447 100644
--- a/logs/rsl_rl/unitree_go2_flat/2024-07-16_15-58-34/exported/policy.pt
+++ b/logs/rsl_rl/unitree_go2_flat/2024-07-16_15-58-34/exported/policy.pt
@@ -1,3 +1,3 @@
 version https://git-lfs.github.com/spec/v1
-oid sha256:d4019c05b8f583accbb59b543853ac876886417a10aad6054f8cff99d7ea9e71
+oid sha256:039d079918cff65df171ff6840f6ff7c92c67fb0b5ecf2c8e19fbbe9f43338e6
 size 173030
diff --git a/source/extensions/omni.isaac.lab_assets/omni/isaac/lab_assets/unitree.py b/source/extensions/omni.isaac.lab_assets/omni/isaac/lab_assets/unitree.py
index 76ab9ce..bddb04f 100644
--- a/source/extensions/omni.isaac.lab_assets/omni/isaac/lab_assets/unitree.py
+++ b/source/extensions/omni.isaac.lab_assets/omni/isaac/lab_assets/unitree.py
@@ -150,11 +150,11 @@ UNITREE_GO2_CFG = ArticulationCfg(
     init_state=ArticulationCfg.InitialStateCfg(
         pos=(0.0, 0.0, 0.4),
         joint_pos={
-            ".*L_hip_joint": 0.1,  # Original Settings from Isaac Lab
-            ".*R_hip_joint": -0.1,
-            "F[L,R]_thigh_joint": 0.8,
-            "R[L,R]_thigh_joint": 1.0,
-            ".*_calf_joint": -1.5,
+            ".*L_hip_joint": 0.0,
+            ".*R_hip_joint": -0.0,
+            "F[L,R]_thigh_joint": 1.1,
+            "R[L,R]_thigh_joint": 1.1,
+            ".*_calf_joint": -1.8,
             # ".*_hip_joint": -0.126,
             # ".*_thigh_joint": 1.22,
             # ".*_calf_joint": -2.7,
diff --git a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/mdp/rewards.py b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
index fb7b763..f3188ad 100644
--- a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
+++ b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
@@ -165,16 +165,16 @@ class GaitReward(ManagerTermBase):
 
 
 def foot_clearance_reward(
-    env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, target_height: float, std: float, tanh_mult: float
+    env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, target_height: float, std: float
 ) -> torch.Tensor:
     """Reward the swinging feet for clearing a specified height off the ground"""
     asset: RigidObject = env.scene[asset_cfg.name]
     foot_z_target_error = torch.square(asset.data.body_pos_w[:, asset_cfg.body_ids, 2] - target_height)
-    foot_velocity_tanh = torch.tanh(tanh_mult * torch.norm(asset.data.body_lin_vel_w[:, asset_cfg.body_ids, :2], dim=2))
-    reward = foot_z_target_error * foot_velocity_tanh
+    reward = foot_z_target_error
     return torch.exp(-torch.sum(reward, dim=1) / std)
 
 
+
 # -- Regularization Penalties
 
 
diff --git a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py
index 618b1f7..d6f2d6d 100644
--- a/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py
+++ b/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py
@@ -100,7 +100,7 @@ class CommandsCfg:
         heading_control_stiffness=0.6,
         debug_vis=True,
         ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
+            lin_vel_x=(-1.5, 1.5), lin_vel_y=(-1.5, 1.5), ang_vel_z=(-1.5, 1.5), heading=(-math.pi, math.pi)
         ),
     )
 
@@ -179,8 +179,8 @@ class EventCfg:
         mode="reset",
         params={
             "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "force_range": (-1.0, 1.0),
-            "torque_range": (-1.0, 1.0),
+            "force_range": (-0.0, 0.0),
+            "torque_range": (-0.0, 0.0),
         },
     )
 
@@ -190,12 +190,12 @@ class EventCfg:
         params={
             "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
             "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
+                "x": (-0.0, 0.0),
+                "y": (-0.0, 0.0),
+                "z": (-0.0, 0.0),
+                "roll": (-0.0, 0.0),
+                "pitch": (-0.0, 0.0),
+                "yaw": (-0.0, 0.0),
             },
         },
     )
@@ -221,14 +221,13 @@ class EventCfg:
 @configclass
 class RewardsCfg:
     """Reward terms for the MDP."""
-    # -- task
+    # FROM SPOT
     air_time = RewardTermCfg(
-        func=mdp.air_time_reward,
-        weight=3.0,
+        func=mdp.feet_air_time,
+        weight=5.0,
         params={
-            "mode_time": 0.3,
-            "velocity_threshold": 0.5,
-            "asset_cfg": SceneEntityCfg("robot"),
+            "threshold": 0.0,
+            "command_name": "base_velocity",
             "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_foot"),
         },
     )
@@ -244,29 +243,25 @@ class RewardsCfg:
     )
     foot_clearance = RewardTermCfg(
         func=mdp.foot_clearance_reward,
-        weight=0.5,
+        weight=5.0,
         params={
-            "std": 0.05,
-            "tanh_mult": 2.0,
-            "target_height": 0.04,
+            "std": 0.01,
+            "target_height": 0.10,
             "asset_cfg": SceneEntityCfg("robot", body_names=".*_foot"),
         },
     )
     gait = RewardTermCfg(
         func=mdp.GaitReward,
-        weight=4.0,
+        weight=5.0,
         params={
-            "std": 0.1,
+            "std": 0.05,
             "max_err": 0.2,
-            "velocity_threshold": 0.5,
+            "velocity_threshold": 0.0,
             "synced_feet_pair_names": (("FL_foot", "RR_foot"), ("FR_foot", "RL_foot")),
             "asset_cfg": SceneEntityCfg("robot"),
             "sensor_cfg": SceneEntityCfg("contact_forces"),
         },
     )
-
-    # -- penalties
-    base_height_l2 = RewardTermCfg(func=mdp.base_height_l2, weight=-1.0, params={'target_height': 0.22})
     action_smoothness = RewardTermCfg(func=mdp.action_smoothness_penalty, weight=-1.0)
     air_time_variance = RewardTermCfg(
         func=mdp.air_time_variance_penalty,
@@ -281,7 +276,7 @@ class RewardsCfg:
     )
     foot_slip = RewardTermCfg(
         func=mdp.foot_slip_penalty,
-        weight=-0.5,
+        weight=-1.5,
         params={
             "asset_cfg": SceneEntityCfg("robot", body_names=".*_foot"),
             "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_foot"),
@@ -295,11 +290,11 @@ class RewardsCfg:
     )
     joint_pos = RewardTermCfg(
         func=mdp.joint_position_penalty,
-        weight=-0.7,
+        weight=-0.3,
         params={
             "asset_cfg": SceneEntityCfg("robot", joint_names=".*"),
             "stand_still_scale": 5.0,
-            "velocity_threshold": 0.5,
+            "velocity_threshold": 0.0,
         },
     )
     joint_torques = RewardTermCfg(
@@ -312,19 +307,28 @@ class RewardsCfg:
         weight=-1.0e-2,
         params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*")},
     )
-    # -- task -- OUTDATED AND SET TO 0
-    track_lin_vel_xy_exp = RewardTermCfg(
-        func=mdp.track_lin_vel_xy_exp, weight=0.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
+    # ADDITIONAL PENALTIES
+    base_height_l2 = RewardTermCfg(
+        func=mdp.base_height_l2,
+        weight=-1.0,
+        params={
+            'target_height': 0.20,
+            "asset_cfg": SceneEntityCfg("robot", body_names="base")
+        },
     )
-    track_ang_vel_z_exp = RewardTermCfg(
-        func=mdp.track_ang_vel_z_exp, weight=0.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
+    flat_orientation_l2 = RewardTermCfg(func=mdp.flat_orientation_l2, weight=-2.5)
+    action_rate_l2 = RewardTermCfg(func=mdp.action_rate_l2, weight=-0.0)
+    undesired_contact_thigh = RewardTermCfg(
+        func=mdp.undesired_contacts,
+        weight=-2.0,
+        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_thigh"), "threshold": 1.0},
     )
-    # -- penalties
-    lin_vel_z_l2 = RewardTermCfg(func=mdp.lin_vel_z_l2, weight=0.0)
-    ang_vel_xy_l2 = RewardTermCfg(func=mdp.ang_vel_xy_l2, weight=0.0)
-    dof_torques_l2 = RewardTermCfg(func=mdp.joint_torques_l2, weight=0.0)
-    dof_acc_l2 = RewardTermCfg(func=mdp.joint_acc_l2, weight=0.0)
-    action_rate_l2 = RewardTermCfg(func=mdp.action_rate_l2, weight=0.0)
+    undesired_contacts_calf = RewardTermCfg(
+        func=mdp.undesired_contacts,
+        weight=-2.0,
+        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_calf"), "threshold": 1.0},
+    )
+    # -- FOR NO ERRORS --
     feet_air_time = RewardTermCfg(
         func=mdp.feet_air_time,
         weight=0.0,
@@ -334,14 +338,8 @@ class RewardsCfg:
             "threshold": 0.5,
         },
     )
-    undesired_contacts = RewardTermCfg(
-        func=mdp.undesired_contacts,
-        weight=-1.0,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_thigh"), "threshold": 1.0},
-    )
-    # -- optional penalties
-    flat_orientation_l2 = RewardTermCfg(func=mdp.flat_orientation_l2, weight=-2.5)
-    dof_pos_limits = RewardTermCfg(func=mdp.joint_pos_limits, weight=0.0)
+    dof_torques_l2 = RewardTermCfg(func=mdp.joint_torques_l2, weight=0.0)
+
 
 @configclass
 class TerminationsCfg:
@@ -352,10 +350,7 @@ class TerminationsCfg:
         func=mdp.illegal_contact,
         params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 0.1},
     )
-    bad_orientation = DoneTerm(
-        func=mdp.bad_orientation,
-        params={'limit_angle': 1.3}
-    )
+
 
 
 @configclass
diff --git a/source/standalone/workflows/rsl_rl/play.py b/source/standalone/workflows/rsl_rl/play.py
index 3c93132..7d600f3 100644
--- a/source/standalone/workflows/rsl_rl/play.py
+++ b/source/standalone/workflows/rsl_rl/play.py
@@ -69,6 +69,7 @@ def main():
     log_root_path = os.path.abspath(log_root_path)
     print(f"[INFO] Loading experiment from directory: {log_root_path}")
     resume_path = get_checkpoint_path(log_root_path, agent_cfg.load_run, agent_cfg.load_checkpoint)
+    # resume_path = get_checkpoint_path(log_root_path, '2024-07-18_10-57-11', agent_cfg.load_checkpoint)
     print(f"[INFO]: Loading model checkpoint from: {resume_path}")
 
     # load previously trained model